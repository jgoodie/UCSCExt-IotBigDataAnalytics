{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Explore MapReduce: Copy and paste these lines to see how to use Map and Reduce functions in Spark. Copy and paste the session output to submit for this Lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "from operator import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the lab3-iot-gendata.txt file for processing as a Resilient Distributed Dataset (RDD)\n",
    "textFile = spark.sparkContext.textFile(\"lab3-iot-gendata.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the histogram\n",
    "### Getting rid of ' ' in the split() function\n",
    "counts = textFile.flatMap(lambda x: x.split())\\\n",
    "    .map(lambda x: (x, 1))\\\n",
    "    .reduceByKey(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77: 5\n",
      "84: 1\n",
      "76: 3\n",
      "68: 1\n",
      "81: 5\n",
      "82: 7\n",
      "103: 2\n",
      "85: 4\n",
      "73: 4\n",
      "63: 2\n",
      "79: 3\n",
      "86: 2\n",
      "101: 1\n",
      "62: 2\n",
      "87: 1\n",
      "74: 3\n",
      "102: 1\n",
      "100: 1\n",
      "104: 2\n",
      "88: 1\n",
      "60: 1\n",
      "65: 4\n",
      "98: 2\n",
      "92: 2\n",
      "90: 6\n",
      "78: 2\n",
      "94: 1\n",
      "93: 1\n",
      "96: 6\n",
      "95: 2\n",
      "64: 3\n",
      "71: 3\n",
      "70: 2\n",
      "61: 2\n",
      "89: 3\n",
      "97: 1\n",
      "75: 2\n",
      "83: 2\n",
      "91: 1\n",
      "99: 2\n",
      "67: 1\n"
     ]
    }
   ],
   "source": [
    "output = counts.collect()\n",
    "for (word, count) in output:\n",
    "    print(\"%s: %i\" % (word, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Explore Filter: Copy and paste these lines to see how to use Map, Filter, and Reduce functions in Spark. Copy and paste the session output to submit for this Lab.\n",
    "\n",
    "#### NOTE 1: Take a screenshot/snapshot or copy and paste the session output to submit for this Lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "textFile = spark.sparkContext.textFile(\"lab3-iot-gendata.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the filtered histogram\n",
    "### Bug here is that we are comparing a string 89 \n",
    "### and not an integer 89\n",
    "counts = textFile.flatMap(lambda x: x.split())\\\n",
    "    .map(lambda x: int(x))\\\n",
    "    .filter(lambda x: x <= 89)\\\n",
    "    .map(lambda x: (x, 1)).reduceByKey(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = counts.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77: 5\n",
      "84: 1\n",
      "76: 3\n",
      "68: 1\n",
      "81: 5\n",
      "82: 7\n",
      "85: 4\n",
      "73: 4\n",
      "63: 2\n",
      "79: 3\n",
      "86: 2\n",
      "62: 2\n",
      "87: 1\n",
      "74: 3\n",
      "88: 1\n",
      "60: 1\n",
      "65: 4\n",
      "78: 2\n",
      "64: 3\n",
      "71: 3\n",
      "70: 2\n",
      "61: 2\n",
      "89: 3\n",
      "75: 2\n",
      "83: 2\n",
      "67: 1\n"
     ]
    }
   ],
   "source": [
    "for (word, count) in output:\n",
    "    print(\"%s: %i\" % (word, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Modify Step #2 with a compound Filter comparison to find a range instead of a threshold (HINT: Try using and/or).  Make sure to correct any bugs in the compound Filter so that the actual values in your output match the range of values that is expected from the modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "textFile = spark.sparkContext.textFile(\"lab3-iot-gendata.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the filtered histogram \n",
    "### Casting x as an int() in the filter() \n",
    "#.filter(lambda x: (int(x) >= 80) & (int(x) <= 89))\\\n",
    "counts = textFile.flatMap(lambda x: x.split())\\\n",
    "    .map(lambda x: int(x))\\\n",
    "    .filter(lambda x: (x >= 80) & (x <= 89))\\\n",
    "    .map(lambda x: (x, 1)).reduceByKey(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = counts.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84: 1\n",
      "81: 5\n",
      "82: 7\n",
      "85: 4\n",
      "86: 2\n",
      "87: 1\n",
      "88: 1\n",
      "89: 3\n",
      "83: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 36744)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.6/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.6/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.6/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python3.6/socketserver.py\", line 721, in __init__\n",
      "    self.handle()\n",
      "  File \"/opt/spark/python/pyspark/accumulators.py\", line 270, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/opt/spark/python/pyspark/accumulators.py\", line 242, in poll\n",
      "    if func():\n",
      "  File \"/opt/spark/python/pyspark/accumulators.py\", line 246, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/opt/spark/python/pyspark/serializers.py\", line 692, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for (word, count) in output:\n",
    "    print(\"%s: %i\" % (word, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
